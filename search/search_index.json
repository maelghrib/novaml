{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NovaML","text":"<p>Build machine learning applications with NovaML; The NEW machine learning library!</p>"},{"location":"#note","title":"Note","text":"<p>This library is for learning purposes and not ready for production use!</p>"},{"location":"#code","title":"Code","text":"<p>Check the code repository on GitHub</p>"},{"location":"#install","title":"Install","text":"<pre><code>pip install novaml\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<pre><code>import numpy as np\nfrom novaml.models import LinearRegression\n\nx = np.array([1.0, 2.0])\ny = np.array([300.0, 500.0])\nw_init = 0\nb_init = 0\nalpha = 1.0e-2\niterations = 10000\nlambd = None\n\nlinear_regression = LinearRegression()\n\nfinal_w, final_b, _, _ = linear_regression.train(\n    x, y, w_init, b_init, alpha, iterations, lambd\n)\n\nyhat = linear_regression.predict(x, final_w, final_b)\n\nprint(f\"ytrain: {y}\")\nprint(f\"yhat: {yhat}\")\n</code></pre>"},{"location":"#examples","title":"Examples","text":"<p>Check the README.md of examples folder.</p>"},{"location":"#license","title":"License","text":"<p>MIT License</p>"},{"location":"reference/summary/","title":"Summary","text":"<ul> <li>novaml<ul> <li>models<ul> <li>linear_regression</li> <li>logistic_regression</li> </ul> </li> <li>utils<ul> <li>feature_scaling</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/novaml/","title":"novaml","text":"<p>This package contains the submodules of the novaml library.</p> Sub-packages <ul> <li><code>models</code>: Provide code for machine learning models.</li> <li><code>utils</code>: Provide utils functions used by models and examples.</li> </ul>"},{"location":"reference/novaml/models/","title":"models","text":"<p>This package contains the models code.</p> <p>Modules:</p> Name Description <code>linear_regression</code> <p>A module that contains linear regression model.</p> <code>logistic_regression</code> <p>A module that contains logistic regression model.</p>"},{"location":"reference/novaml/models/linear_regression/","title":"linear_regression","text":"<p>A module that contains linear regression model.</p> <p>Classes:</p> Name Description <code>LinearRegression</code> <p>A class that hold the linear regression functions.</p>"},{"location":"reference/novaml/models/linear_regression/#novaml.models.linear_regression.LinearRegression","title":"<code>LinearRegression</code>","text":"<p>The linear regression model for training and predicting linear models.</p> <p>Methods:</p> Name Description <code>_model</code> <p>Private method to calculate the model.</p> <code>_cost</code> <p>Private method to calculate the cost function.</p> <code>_gradient_descent</code> <p>Private method to calculate the gradient decent.</p> <code>train</code> <p>Public method to train X and Y to get the wights and biases.</p> <code>predict</code> <p>Public method to predict Y based on X and the wights and biases.</p> Source code in <code>novaml/models/linear_regression.py</code> <pre><code>class LinearRegression:\n    \"\"\"The linear regression model for training and predicting linear models.\n\n       Methods:\n           _model: Private method to calculate the model.\n           _cost: Private method to calculate the cost function.\n           _gradient_descent: Private method to calculate the gradient decent.\n           train: Public method to train X and Y to get the wights and biases.\n           predict: Public method to predict Y based on X and the wights and biases.\n    \"\"\"\n\n    def _model(\n            self,\n            x: np.ndarray,\n            w: np.ndarray | float,\n            b: np.ndarray | float,\n    ) -&gt; np.ndarray | float:\n        \"\"\"Calculates the linear model.\n\n        Args:\n            x: The x train data.\n            w: The weight.\n            b: The bias.\n\n        Returns:\n            The value of f(x).\n        \"\"\"\n        fx = np.dot(x, w) + b\n        return fx\n\n    def _cost(\n            self,\n            x: np.ndarray,\n            y: np.ndarray,\n            w: np.ndarray | float,\n            b: np.ndarray | float,\n            lambd=None,\n    ) -&gt; np.ndarray | float:\n        \"\"\"Calculate the cost function.\n\n        Args:\n            x: The x train data.\n            y: The y train data.\n            w: The weight.\n            b: The bias.\n            lambd: The regularization term.\n\n        Returns:\n            The cost function.\n        \"\"\"\n        m = x.shape[0]\n        fx = self._model(x, w, b)\n        jwb = np.sum(np.square(fx - y)) / (2 * m)\n        if lambd:\n            jwb += lambd * np.sum(w ** 2) / (2 * m)\n        return jwb\n\n    def _gradient_descent(\n            self,\n            x: np.ndarray,\n            y: np.ndarray,\n            w: np.ndarray | float,\n            b: np.ndarray | float,\n            lambd=None,\n    ) -&gt; (np.ndarray | float, np.ndarray | float):\n        \"\"\"Calculate the gradient descent derivatives.\n\n        Args:\n            x: The x train data.\n            y: The y train data.\n            w: The weight.\n            b: The bias.\n            lambd: The regularization term.\n\n        Returns:\n            The weight derivative and the bias derivative.\n        \"\"\"\n        m = x.shape[0]\n        fx = self._model(x, w, b)\n        dw = np.sum(np.dot((fx - y), x)) / m\n        if lambd:\n            dw += lambd * w / m\n        db = np.sum(fx - y) / m\n        return dw, db\n\n    def train(\n            self,\n            x: np.ndarray,\n            y: np.ndarray,\n            w_init: np.ndarray | float,\n            b_init: np.ndarray | float,\n            alpha: float,\n            iterations: int,\n            lambd: float | None = None,\n    ) -&gt; (np.ndarray | float, np.ndarray | float, list, list):\n        \"\"\"Train the model to calculate the final weight and bias.\n\n        Args:\n            x: The x train data.\n            y: The y train data.\n            w_init: The initial weight.\n            b_init: The initial bias.\n            alpha: The learning rate.\n            iterations: The number of iterations.\n            lambd: The regularization term.\n\n        Returns:\n            The final weight, final bias, cost history, and parameters history.\n        \"\"\"\n        cost_history = []\n        parameters_history = []\n\n        w = copy.deepcopy(w_init)\n        b = copy.deepcopy(b_init)\n\n        for i in range(iterations):\n            dw, db = self._gradient_descent(x, y, w, b, lambd)\n            w = w - alpha * dw\n            b = b - alpha * db\n\n            if i &lt; 100000:\n                cost_history.append(self._cost(x, y, w, b, lambd))\n                parameters_history.append([w, b])\n\n        return w, b, cost_history, parameters_history\n\n    def predict(\n            self,\n            x: np.ndarray | float,\n            w: np.ndarray | float,\n            b: np.ndarray | float,\n    ) -&gt; np.ndarray | float:\n        \"\"\"Predict the new Yhat given the X and final weight and bias.\n\n        Args:\n            x: The x train data.\n            w: The final weight.\n            b: The final bias.\n\n        Returns:\n            The predicted yhat.\n        \"\"\"\n        yhat = self._model(x, w, b)\n        return yhat\n</code></pre>"},{"location":"reference/novaml/models/linear_regression/#novaml.models.linear_regression.LinearRegression.predict","title":"<code>predict(x, w, b)</code>","text":"<p>Predict the new Yhat given the X and final weight and bias.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray | float</code> <p>The x train data.</p> required <code>w</code> <code>ndarray | float</code> <p>The final weight.</p> required <code>b</code> <code>ndarray | float</code> <p>The final bias.</p> required <p>Returns:</p> Type Description <code>ndarray | float</code> <p>The predicted yhat.</p> Source code in <code>novaml/models/linear_regression.py</code> <pre><code>def predict(\n        self,\n        x: np.ndarray | float,\n        w: np.ndarray | float,\n        b: np.ndarray | float,\n) -&gt; np.ndarray | float:\n    \"\"\"Predict the new Yhat given the X and final weight and bias.\n\n    Args:\n        x: The x train data.\n        w: The final weight.\n        b: The final bias.\n\n    Returns:\n        The predicted yhat.\n    \"\"\"\n    yhat = self._model(x, w, b)\n    return yhat\n</code></pre>"},{"location":"reference/novaml/models/linear_regression/#novaml.models.linear_regression.LinearRegression.train","title":"<code>train(x, y, w_init, b_init, alpha, iterations, lambd=None)</code>","text":"<p>Train the model to calculate the final weight and bias.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The x train data.</p> required <code>y</code> <code>ndarray</code> <p>The y train data.</p> required <code>w_init</code> <code>ndarray | float</code> <p>The initial weight.</p> required <code>b_init</code> <code>ndarray | float</code> <p>The initial bias.</p> required <code>alpha</code> <code>float</code> <p>The learning rate.</p> required <code>iterations</code> <code>int</code> <p>The number of iterations.</p> required <code>lambd</code> <code>float | None</code> <p>The regularization term.</p> <code>None</code> <p>Returns:</p> Type Description <code>(ndarray | float, ndarray | float, list, list)</code> <p>The final weight, final bias, cost history, and parameters history.</p> Source code in <code>novaml/models/linear_regression.py</code> <pre><code>def train(\n        self,\n        x: np.ndarray,\n        y: np.ndarray,\n        w_init: np.ndarray | float,\n        b_init: np.ndarray | float,\n        alpha: float,\n        iterations: int,\n        lambd: float | None = None,\n) -&gt; (np.ndarray | float, np.ndarray | float, list, list):\n    \"\"\"Train the model to calculate the final weight and bias.\n\n    Args:\n        x: The x train data.\n        y: The y train data.\n        w_init: The initial weight.\n        b_init: The initial bias.\n        alpha: The learning rate.\n        iterations: The number of iterations.\n        lambd: The regularization term.\n\n    Returns:\n        The final weight, final bias, cost history, and parameters history.\n    \"\"\"\n    cost_history = []\n    parameters_history = []\n\n    w = copy.deepcopy(w_init)\n    b = copy.deepcopy(b_init)\n\n    for i in range(iterations):\n        dw, db = self._gradient_descent(x, y, w, b, lambd)\n        w = w - alpha * dw\n        b = b - alpha * db\n\n        if i &lt; 100000:\n            cost_history.append(self._cost(x, y, w, b, lambd))\n            parameters_history.append([w, b])\n\n    return w, b, cost_history, parameters_history\n</code></pre>"},{"location":"reference/novaml/models/logistic_regression/","title":"logistic_regression","text":"<p>A module that contains logistic regression model.</p> <p>Classes:</p> Name Description <code>LogisticRegression</code> <p>A class that hold the logistic regression functions.</p>"},{"location":"reference/novaml/models/logistic_regression/#novaml.models.logistic_regression.LogisticRegression","title":"<code>LogisticRegression</code>","text":"<p>The logistic regression model for classification.</p> <p>Methods:</p> Name Description <code>_model</code> <p>Private method to calculate the model.</p> <code>_cost</code> <p>Private method to calculate the cost function.</p> <code>_gradient_descent</code> <p>Private method to calculate the gradient decent.</p> <code>train</code> <p>Public method to train X and Y to get the wights and biases.</p> <code>predict</code> <p>Public method to predict Y based on X and the wights and biases.</p> Source code in <code>novaml/models/logistic_regression.py</code> <pre><code>class LogisticRegression:\n    \"\"\"The logistic regression model for classification.\n\n       Methods:\n           _model: Private method to calculate the model.\n           _cost: Private method to calculate the cost function.\n           _gradient_descent: Private method to calculate the gradient decent.\n           train: Public method to train X and Y to get the wights and biases.\n           predict: Public method to predict Y based on X and the wights and biases.\n    \"\"\"\n\n    def _model(\n            self,\n            x: np.ndarray | float,\n            w: np.ndarray | float,\n            b: np.ndarray | float,\n    ) -&gt; np.ndarray | float:\n        \"\"\"Calculates the sigmoid function.\n\n        Args:\n            x: The x train data\n            w: The weight\n            b: The bias\n\n        Returns:\n            The value of f(x)\n        \"\"\"\n        z = np.dot(x, w) + b\n        fx = 1 / (1 + np.exp(-z))\n        return fx\n\n    def _cost(\n            self,\n            x: np.ndarray,\n            y: np.ndarray,\n            w: np.ndarray | float,\n            b: np.ndarray | float,\n            lambd: float | None = None,\n    ) -&gt; np.ndarray | float:\n        \"\"\"Calculates the cost function.\n\n        Args:\n            x: The x train data\n            y: The y train data\n            w: The weight\n            b: The bias\n            lambd: The regularization term\n\n        Returns:\n            The cost function.\n        \"\"\"\n        m = x.shape[0]\n        fx = self._model(x, w, b)\n        jwb = -1 * np.sum(np.dot(y, np.log(fx)) + np.dot((1 - y), np.log(1 - fx))) / m\n        if lambd:\n            jwb += lambd * np.sum(np.square(w)) / (2 * m)\n        return jwb\n\n    def _gradient_descent(\n            self,\n            x: np.ndarray,\n            y: np.ndarray,\n            w: np.ndarray | float,\n            b: np.ndarray | float,\n            lambd: float | None = None,\n    ) -&gt; (np.ndarray | float, np.ndarray | float):\n        \"\"\"Calculate the gradient descent derivatives.\n\n        Args:\n            x: The x train data\n            y: The y train data\n            w: The weight\n            b: The bias\n            lambd: The regularization term\n\n        Returns:\n            The weight derivative and the bias derivative.\n        \"\"\"\n        m = x.shape[0]\n        fx = self._model(x, w, b)\n        dw = np.sum(np.dot((fx.T - y), x)) / m\n        if lambd:\n            dw += lambd * w / m\n        db = np.sum(fx.T - y) / m\n        return dw, db\n\n    def train(\n            self,\n            x: np.ndarray,\n            y: np.ndarray,\n            w_init: np.ndarray | float,\n            b_init: np.ndarray | float,\n            alpha: float,\n            iterations: int,\n            lambd: float | None = None,\n    ) -&gt; (np.ndarray | float, np.ndarray | float, list, list):\n        \"\"\"Train the model to calculate the final weight and bias.\n\n        Args:\n            x: The x train data.\n            y: The y train data.\n            w_init: The initial weight.\n            b_init: The initial bias.\n            alpha: The learning rate.\n            iterations: The number of iterations.\n            lambd: The regularization term.\n\n        Returns:\n            The final weight, final bias, cost history, and parameters history.\n        \"\"\"\n        cost_history = []\n        parameters_history = []\n\n        w = copy.deepcopy(w_init)\n        b = copy.deepcopy(b_init)\n\n        for i in range(iterations):\n            dw, db = self._gradient_descent(x, y, w, b, lambd)\n            w = w - alpha * dw\n            b = b - alpha * db\n\n            if i &lt; 100000:\n                cost_history.append(self._cost(x, y, w, b, lambd))\n                parameters_history.append([w, b])\n\n        return w, b, cost_history, parameters_history\n\n    def predict(\n            self,\n            x: np.ndarray | float,\n            w: np.ndarray | float,\n            b: np.ndarray | float,\n    ) -&gt; np.ndarray | float:\n        \"\"\"Predict the new Yhat given the X and final weight and bias.\n\n        Args:\n            x: The x train data.\n            w: The final weight.\n            b: The final bias.\n\n        Returns:\n            The predicted yhat.\n        \"\"\"\n        yhat = self._model(x, w, b)\n        return yhat\n</code></pre>"},{"location":"reference/novaml/models/logistic_regression/#novaml.models.logistic_regression.LogisticRegression.predict","title":"<code>predict(x, w, b)</code>","text":"<p>Predict the new Yhat given the X and final weight and bias.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray | float</code> <p>The x train data.</p> required <code>w</code> <code>ndarray | float</code> <p>The final weight.</p> required <code>b</code> <code>ndarray | float</code> <p>The final bias.</p> required <p>Returns:</p> Type Description <code>ndarray | float</code> <p>The predicted yhat.</p> Source code in <code>novaml/models/logistic_regression.py</code> <pre><code>def predict(\n        self,\n        x: np.ndarray | float,\n        w: np.ndarray | float,\n        b: np.ndarray | float,\n) -&gt; np.ndarray | float:\n    \"\"\"Predict the new Yhat given the X and final weight and bias.\n\n    Args:\n        x: The x train data.\n        w: The final weight.\n        b: The final bias.\n\n    Returns:\n        The predicted yhat.\n    \"\"\"\n    yhat = self._model(x, w, b)\n    return yhat\n</code></pre>"},{"location":"reference/novaml/models/logistic_regression/#novaml.models.logistic_regression.LogisticRegression.train","title":"<code>train(x, y, w_init, b_init, alpha, iterations, lambd=None)</code>","text":"<p>Train the model to calculate the final weight and bias.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The x train data.</p> required <code>y</code> <code>ndarray</code> <p>The y train data.</p> required <code>w_init</code> <code>ndarray | float</code> <p>The initial weight.</p> required <code>b_init</code> <code>ndarray | float</code> <p>The initial bias.</p> required <code>alpha</code> <code>float</code> <p>The learning rate.</p> required <code>iterations</code> <code>int</code> <p>The number of iterations.</p> required <code>lambd</code> <code>float | None</code> <p>The regularization term.</p> <code>None</code> <p>Returns:</p> Type Description <code>(ndarray | float, ndarray | float, list, list)</code> <p>The final weight, final bias, cost history, and parameters history.</p> Source code in <code>novaml/models/logistic_regression.py</code> <pre><code>def train(\n        self,\n        x: np.ndarray,\n        y: np.ndarray,\n        w_init: np.ndarray | float,\n        b_init: np.ndarray | float,\n        alpha: float,\n        iterations: int,\n        lambd: float | None = None,\n) -&gt; (np.ndarray | float, np.ndarray | float, list, list):\n    \"\"\"Train the model to calculate the final weight and bias.\n\n    Args:\n        x: The x train data.\n        y: The y train data.\n        w_init: The initial weight.\n        b_init: The initial bias.\n        alpha: The learning rate.\n        iterations: The number of iterations.\n        lambd: The regularization term.\n\n    Returns:\n        The final weight, final bias, cost history, and parameters history.\n    \"\"\"\n    cost_history = []\n    parameters_history = []\n\n    w = copy.deepcopy(w_init)\n    b = copy.deepcopy(b_init)\n\n    for i in range(iterations):\n        dw, db = self._gradient_descent(x, y, w, b, lambd)\n        w = w - alpha * dw\n        b = b - alpha * db\n\n        if i &lt; 100000:\n            cost_history.append(self._cost(x, y, w, b, lambd))\n            parameters_history.append([w, b])\n\n    return w, b, cost_history, parameters_history\n</code></pre>"},{"location":"reference/novaml/utils/","title":"utils","text":"<p>This package contains the utils code.</p> <p>Modules:</p> Name Description <code>feature_scaling</code> <p>A module that contains feature scaling functions.</p>"},{"location":"reference/novaml/utils/feature_scaling/","title":"feature_scaling","text":"<p>A module that contains feature scaling utils functions.</p> <p>Functions:</p> Name Description <code>divide_by_max</code> <p>A function that scale x by dividing by max of x.</p> <code>mean_normalization</code> <p>A function that scale x by using mean.</p> <code>z_score_normalization</code> <p>A function that scale x by using mean and standard deviation.</p>"},{"location":"reference/novaml/utils/feature_scaling/#novaml.utils.feature_scaling.divide_by_max","title":"<code>divide_by_max(x)</code>","text":"<p>Scale x by dividing by max of x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The x data to be scaled.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The scaled x.</p> Source code in <code>novaml/utils/feature_scaling.py</code> <pre><code>def divide_by_max(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Scale x by dividing by max of x.\n\n    Args:\n        x: The x data to be scaled.\n\n    Returns:\n        The scaled x.\n    \"\"\"\n    x_scaled = (x - x.min()) / x.max()\n    return x_scaled\n</code></pre>"},{"location":"reference/novaml/utils/feature_scaling/#novaml.utils.feature_scaling.mean_normalization","title":"<code>mean_normalization(x)</code>","text":"<p>Scale x by using mean.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The x data to be scaled.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The scaled x.</p> Source code in <code>novaml/utils/feature_scaling.py</code> <pre><code>def mean_normalization(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Scale x by using mean.\n\n    Args:\n        x: The x data to be scaled.\n\n    Returns:\n        The scaled x.\n    \"\"\"\n    mu = np.mean(x, axis=0)\n    x_scaled = (x - mu) / (x.max() - x.min())\n    return x_scaled\n</code></pre>"},{"location":"reference/novaml/utils/feature_scaling/#novaml.utils.feature_scaling.z_score_normalization","title":"<code>z_score_normalization(x)</code>","text":"<p>Scale x by using mean and standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The x data to be scaled.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The scaled x.</p> Source code in <code>novaml/utils/feature_scaling.py</code> <pre><code>def z_score_normalization(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Scale x by using mean and standard deviation.\n\n    Args:\n        x: The x data to be scaled.\n\n    Returns:\n        The scaled x.\n    \"\"\"\n    mu = np.mean(x, axis=0)\n    sigma = np.std(x, axis=0)\n    x_scaled = (x - mu) / sigma\n    return x_scaled\n</code></pre>"}]}